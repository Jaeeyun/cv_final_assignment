{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1c4HnAVX7XCGQTUCE_JtQda-lcIm1hApO",
      "authorship_tag": "ABX9TyNN+uAotCGa9e2It5qFcQE0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaeeyun/cv_final_assignment/blob/main/final_cv_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Drive 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51QtR7_sys0E",
        "outputId": "5893ef83-13c3-44f2-a103-ce484f49bd60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import time\n",
        "import copy\n",
        "import os\n",
        "import cv2 # cv2는 이미지 로드 시 PIL로 대체하거나 keep 가능\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# --- 설정 ---\n",
        "TRAIN_DATA_DIR = '/content/drive/MyDrive/cv_final/POC_Dataset/Training'\n",
        "NUM_CLASSES = 4 # 데이터셋의 클래스 수 (0, 1, 2, 3)\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.0001 # 파인튜닝에 적합한 낮은 학습률\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 1. DataFrame 로드 함수 (이전과 동일)\n",
        "def create_dataframe_and_encode_labels(base_dir):\n",
        "    base_dir = Path(base_dir)\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "    print(f\"'{base_dir}' 디렉토리에서 데이터 로딩 중...\")\n",
        "    for class_dir in sorted(base_dir.iterdir()):\n",
        "        if class_dir.is_dir() and not class_dir.name.startswith('.'):\n",
        "            label_name = class_dir.name\n",
        "            for image_file in class_dir.glob('*'):\n",
        "                if image_file.suffix.lower() in ['.png', '.jpg', '.jpeg', '.tif', '.tiff']:\n",
        "                    image_paths.append(str(image_file.resolve()))\n",
        "                    labels.append(label_name)\n",
        "    df = pd.DataFrame({'image_path': image_paths, 'label_name': labels})\n",
        "    le = LabelEncoder()\n",
        "    df['label'] = le.fit_transform(df['label_name'])\n",
        "    print(f\"\\n총 {len(df)}개의 이미지 경로와 라벨을 매핑했습니다.\")\n",
        "    print(f\"클래스 매핑: {list(le.classes_)}\")\n",
        "    return df\n",
        "\n",
        "# 2. DataFrame 로드 및 분할\n",
        "df = create_dataframe_and_encode_labels(base_dir=TRAIN_DATA_DIR)\n",
        "train_df, val_df = train_test_split(df, test_size=0.3, stratify=df['label'], random_state=42)\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "val_df = val_df.reset_index(drop=True)\n",
        "\n",
        "# 3. 사용자 정의 Dataset 클래스 (PyTorch 및 PIL 사용)\n",
        "class PathologyDataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.dataframe.iloc[idx]['image_path']\n",
        "        label = self.dataframe.iloc[idx]['label']\n",
        "\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "\n",
        "# 4. PyTorch Transforms 및 DataLoader 설정 (AlexNet 표준)\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "\n",
        "        transforms.RandomHorizontalFlip(p=0.5), # 수평 뒤집기\n",
        "        transforms.RandomVerticalFlip(p=0.5),   # 수직 뒤집기\n",
        "        transforms.RandomRotation(degrees=30),  # -30도에서 +30도 사이 랜덤 회전\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1), # 밝기, 대비 등 랜덤 조절\n",
        "\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "train_dataset = PathologyDataset(train_df, transform=data_transforms['train'])\n",
        "val_dataset = PathologyDataset(val_df, transform=data_transforms['val'])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "dataloaders = {'train': train_loader, 'val': val_loader}\n",
        "dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n",
        "\n",
        "\n",
        "# 5. AlexNet 모델 로드 및 수정\n",
        "model_ft = models.alexnet(weights=models.AlexNet_Weights.DEFAULT)\n",
        "num_ftrs = model_ft.classifier[6].in_features # AlexNet 마지막 FC 레이어 입력 특성 수\n",
        "model_ft.classifier[6] = nn.Linear(num_ftrs, NUM_CLASSES) # 새 출력 레이어로 교체\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "\n",
        "# 6. 손실 함수, 최적화 도구, 스케줄러 설정\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_ft = optim.Adam(model_ft.parameters(), lr=LEARNING_RATE)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "\n",
        "\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=NUM_EPOCHS):\n",
        "    since = time.time()\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            progress_bar = tqdm(dataloaders[phase], desc=f'Epoch {epoch+1} {phase}')\n",
        "\n",
        "            for inputs, labels in progress_bar:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "                progress_bar.set_postfix(loss=loss.item(), acc=torch.sum(preds == labels.data).item()/inputs.size(0))\n",
        "\n",
        "\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'학습 완료 시간: {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'최고 검증 정확도: {best_acc:.4f}')\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "\n",
        "# 8. 학습 실행\n",
        "print(\"\\n--- AlexNet 모델 파인튜닝 시작 ---\")\n",
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=NUM_EPOCHS)\n",
        "print(\"--- 학습 완료 ---\")\n",
        "\n",
        "# 모델 저장\n",
        "torch.save(model_ft.state_dict(), '/content/drive/MyDrive/cv_final/alexnet_pathology_fine_tuned_model_2.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3urqOKQx8hsm",
        "outputId": "02127af6-ad57-447d-e4b4-a4091d08d054"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'/content/drive/MyDrive/cv_final/POC_Dataset/Training' 디렉토리에서 데이터 로딩 중...\n",
            "\n",
            "총 4167개의 이미지 경로와 라벨을 매핑했습니다.\n",
            "클래스 매핑: ['Chorionic_villi', 'Decidual_tissue', 'Hemorrhage', 'Trophoblastic_tissue']\n",
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 233M/233M [00:01<00:00, 159MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- AlexNet 모델 파인튜닝 시작 ---\n",
            "Epoch 1/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 train: 100%|██████████| 92/92 [07:54<00:00,  5.16s/it, acc=0.25, loss=1.1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.5930 Acc: 0.7781\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 val: 100%|██████████| 40/40 [03:00<00:00,  4.50s/it, acc=1, loss=0.04]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.2906 Acc: 0.8793\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 train: 100%|██████████| 92/92 [06:23<00:00,  4.16s/it, acc=0.75, loss=0.226]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.3809 Acc: 0.8549\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 val: 100%|██████████| 40/40 [00:50<00:00,  1.26s/it, acc=1, loss=0.0151]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.3117 Acc: 0.8777\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 train: 100%|██████████| 92/92 [06:15<00:00,  4.08s/it, acc=1, loss=0.0105]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.3350 Acc: 0.8765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 val: 100%|██████████| 40/40 [00:49<00:00,  1.24s/it, acc=1, loss=0.0159]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.2791 Acc: 0.8897\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 train: 100%|██████████| 92/92 [06:15<00:00,  4.09s/it, acc=0.75, loss=0.45]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.2888 Acc: 0.8916\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 val: 100%|██████████| 40/40 [00:50<00:00,  1.27s/it, acc=1, loss=0.0239]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.2438 Acc: 0.9121\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 train: 100%|██████████| 92/92 [06:06<00:00,  3.99s/it, acc=1, loss=0.163]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.2964 Acc: 0.8796\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 val: 100%|██████████| 40/40 [00:51<00:00,  1.28s/it, acc=1, loss=0.0468]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.2320 Acc: 0.9161\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6 train: 100%|██████████| 92/92 [06:09<00:00,  4.02s/it, acc=1, loss=0.0316]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.2688 Acc: 0.8999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6 val: 100%|██████████| 40/40 [00:51<00:00,  1.28s/it, acc=1, loss=0.00378]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.1946 Acc: 0.9185\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7 train: 100%|██████████| 92/92 [06:10<00:00,  4.02s/it, acc=1, loss=0.0044]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.2220 Acc: 0.9122\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7 val: 100%|██████████| 40/40 [00:49<00:00,  1.24s/it, acc=1, loss=0.0171]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.1940 Acc: 0.9289\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8 train: 100%|██████████| 92/92 [06:12<00:00,  4.04s/it, acc=1, loss=0.136]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.1880 Acc: 0.9228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8 val: 100%|██████████| 40/40 [00:49<00:00,  1.23s/it, acc=1, loss=0.00526]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.1672 Acc: 0.9353\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9 train: 100%|██████████| 92/92 [06:13<00:00,  4.06s/it, acc=1, loss=0.00942]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.1675 Acc: 0.9314\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9 val: 100%|██████████| 40/40 [00:51<00:00,  1.28s/it, acc=1, loss=0.00272]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.1656 Acc: 0.9345\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10 train: 100%|██████████| 92/92 [06:11<00:00,  4.04s/it, acc=1, loss=0.000546]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.1550 Acc: 0.9366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10 val: 100%|██████████| 40/40 [00:50<00:00,  1.25s/it, acc=1, loss=0.00361]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.1660 Acc: 0.9329\n",
            "\n",
            "학습 완료 시간: 74m 28s\n",
            "최고 검증 정확도: 0.9353\n",
            "--- 학습 완료 ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "TEST_DATA_DIR = '/content/drive/MyDrive/cv_final/POC_Dataset/Testing'\n",
        "\n",
        "# 학습 시 사용한 create_dataframe_and_encode_labels 함수가 메모리에 있다고 가정\n",
        "# 학습 시 사용한 df (학습+검증 전체 데이터프레임)를 사용하여 레이블 매핑을 일관되게 유지\n",
        "\n",
        "# 1. 테스트 데이터셋 DataFrame 생성\n",
        "print(f\"\\n--- '{TEST_DATA_DIR}' 디렉토리에서 테스트 데이터 로딩 중 ---\")\n",
        "# create_dataframe_and_encode_labels 함수는 LabelEncoder를 사용하므로 일관된 인코딩 보장\n",
        "test_df = create_dataframe_and_encode_labels(base_dir=TEST_DATA_DIR)\n",
        "\n",
        "# 2. 테스트 데이터셋 인스턴스 생성\n",
        "# 학습 시 정의한 data_transforms['val'] (검증용 변환) 사용\n",
        "test_dataset = PathologyDataset(test_df, transform=data_transforms['val']) # PathologyDataset 클래스 필요\n",
        "\n",
        "# 3. 테스트 DataLoader 생성\n",
        "TEST_BATCH_SIZE = 32\n",
        "test_loader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "# 4. 테스트 함수 정의\n",
        "def test_model(model, dataloader, device):\n",
        "    model.eval() # 모델을 평가 모드로 설정\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    print(\"\\n--- 테스트 데이터셋에 대한 예측 수행 중 ---\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # gradient 계산을 비활성화하여 메모리 절약 및 속도 향상\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            # 가장 높은 확률을 가진 클래스 인덱스 가져오기\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            # 결과를 CPU로 옮기고 리스트에 저장\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    time_elapsed = time.time() - start_time\n",
        "    print(f\"예측 완료 시간: {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\")\n",
        "\n",
        "    return np.array(true_labels), np.array(predictions)\n",
        "\n",
        "# 5. 예측 실행\n",
        "# 학습이 완료된 model_ft 변수 사용\n",
        "labels, preds = test_model(model_ft, test_loader, device)\n",
        "\n",
        "\n",
        "# 6. 결과 DataFrame에 추가 및 확인\n",
        "test_df['predicted_label'] = preds\n",
        "# 숫자 라벨을 원래 클래스 이름으로 다시 매핑 (학습 데이터의 클래스 매핑 순서를 따름)\n",
        "# 학습 코드에서 사용한 'df' (전체 학습 데이터프레임) 사용\n",
        "label_map = dict(zip(df['label'], df['label_name']))\n",
        "test_df['predicted_label_name'] = test_df['predicted_label'].map(label_map)\n",
        "\n",
        "\n",
        "print(\"\\n--- 예측 결과 확인 (샘플 5개) ---\")\n",
        "print(test_df[['image_path', 'label_name', 'predicted_label_name']].head())\n",
        "\n",
        "\n",
        "# 7. 성능 평가 및 리포트 출력\n",
        "if labels is not None and len(labels) == len(preds):\n",
        "    print(\"\\n--- 전체 테스트 데이터셋 성능 리포트 ---\")\n",
        "\n",
        "    # 총 예측 개수 계산\n",
        "    total_count = len(labels)\n",
        "    # 맞은 개수 계산\n",
        "    correct_count = (preds == labels).sum()\n",
        "    # 정확도 계산\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "\n",
        "    print(f\"총 이미지 개수: {total_count}개\")\n",
        "    print(f\"정답 개수: {correct_count}개\")\n",
        "    print(f\"정확도 (Accuracy): {accuracy:.4f}\\n\")\n",
        "\n",
        "    # 정밀도, 재현율, F1-스코어 계산\n",
        "    # target_names 클래스 이름 목록\n",
        "    target_names = list(df['label_name'].unique())\n",
        "    print(classification_report(labels, preds, target_names=target_names))\n",
        "else:\n",
        "    print(\"\\n테스트 데이터셋에 실제 라벨이 없거나 길이가 맞지 않아 성능 리포트를 생성할 수 없습니다.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_STvmG0Z-Xcx",
        "outputId": "15fca918-b0ba-45ef-a0b4-0090aca4847c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- '/content/drive/MyDrive/cv_final/POC_Dataset/Testing' 디렉토리에서 테스트 데이터 로딩 중 ---\n",
            "'/content/drive/MyDrive/cv_final/POC_Dataset/Testing' 디렉토리에서 데이터 로딩 중...\n",
            "\n",
            "총 1511개의 이미지 경로와 라벨을 매핑했습니다.\n",
            "클래스 매핑: ['Chorionic_villi', 'Decidual_tissue', 'Hemorrhage', 'Trophoblastic_tissue']\n",
            "\n",
            "--- 테스트 데이터셋에 대한 예측 수행 중 ---\n",
            "예측 완료 시간: 3m 47s\n",
            "\n",
            "--- 예측 결과 확인 (샘플 5개) ---\n",
            "                                          image_path       label_name  \\\n",
            "0  /content/drive/MyDrive/cv_final/POC_Dataset/Te...  Chorionic_villi   \n",
            "1  /content/drive/MyDrive/cv_final/POC_Dataset/Te...  Chorionic_villi   \n",
            "2  /content/drive/MyDrive/cv_final/POC_Dataset/Te...  Chorionic_villi   \n",
            "3  /content/drive/MyDrive/cv_final/POC_Dataset/Te...  Chorionic_villi   \n",
            "4  /content/drive/MyDrive/cv_final/POC_Dataset/Te...  Chorionic_villi   \n",
            "\n",
            "  predicted_label_name  \n",
            "0      Chorionic_villi  \n",
            "1      Chorionic_villi  \n",
            "2      Chorionic_villi  \n",
            "3      Chorionic_villi  \n",
            "4      Chorionic_villi  \n",
            "\n",
            "--- 전체 테스트 데이터셋 성능 리포트 ---\n",
            "총 이미지 개수: 1511개\n",
            "정답 개수: 1306개\n",
            "정확도 (Accuracy): 0.8643\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     Chorionic_villi       0.91      0.98      0.95       390\n",
            "     Decidual_tissue       0.84      0.63      0.72       349\n",
            "          Hemorrhage       0.78      0.96      0.86       421\n",
            "Trophoblastic_tissue       0.96      0.85      0.90       351\n",
            "\n",
            "            accuracy                           0.86      1511\n",
            "           macro avg       0.87      0.86      0.86      1511\n",
            "        weighted avg       0.87      0.86      0.86      1511\n",
            "\n"
          ]
        }
      ]
    }
  ]
}